# 安装依赖
!pip install peft transformers datasets

# 加载预训练模型和分词器
from transformers import AutoModelForCausalLM, AutoTokenizer

model_name = "Qwen/Qwen1.5-14B"
model = AutoModelForCausalLM.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)

# 配置 LoRA
from peft import LoraConfig, get_peft_model

lora_config = LoraConfig(
    r=16,  # 低秩分解的秩
    lora_alpha=32,  # 缩放因子
    target_modules=["q_proj", "v_proj"],  # 目标层（注意力层的权重矩阵）
    lora_dropout=0.1,  # Dropout 概率
    bias="none",  # 是否训练偏置项
    task_type="CAUSAL_LM"  # 任务类型（因果语言模型）
)

model = get_peft_model(model, lora_config)

# 准备数据集
from datasets import load_dataset

dataset = load_dataset("your_dataset_name")
tokenized_dataset = dataset.map(lambda x: tokenizer(x["text"], truncation=True, padding="max_length"), batched=True)

# 设置训练参数
from transformers import TrainingArguments, Trainer

training_args = TrainingArguments(
    output_dir="./results",
    per_device_train_batch_size=4,
    per_device_eval_batch_size=4,
    num_train_epochs=3,
    logging_dir="./logs",
    logging_steps=10,
    save_steps=500,
    evaluation_strategy="steps",
    eval_steps=500,
    learning_rate=5e-5,
    fp16=True,  # 使用混合精度训练
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset["train"],
    eval_dataset=tokenized_dataset["validation"],
)

# 开始训练
trainer.train()

# 保存模型
model.save_pretrained("./fine_tuned_model")
tokenizer.save_pretrained("./fine_tuned_model")

# 测试集评估
eval_results = trainer.evaluate(tokenized_dataset["test"])
print(eval_results)